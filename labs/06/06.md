---
marp: true
theme: gaia
class: lead
backgroundColor: #fff
color: #333
paginate: true
header: "Advanced AI Engineering"
footer: "State of the Art 2025"
---

<!-- _class: invert -->

# Parrots to Cognitive Architectures
## AI Engineering in 2025


---

# Lecture Overview

1.  **The Problem:** The Infinite Software Crisis & "Slop"
2.  **Architecture:** The Harness, Skills, and Context Engineering
3.  **Training:** Moving from Prompting to Reinforcement Fine-Tuning (RFT)
4.  **Modeling:** Execution Traces vs. Syntax Prediction
5.  **Evaluation:** The Benchmark Paradox & Reward Hacking
6.  **Interface:** The Death of the IDE

---

<!-- _class: invert -->

# Part 1: The Paradigm Shift
## The "Harness" is the new Moat

---

## The Infinite Software Crisis

*   **Economic Shift:** Marginal cost of generating code has hit zero.
*   **Consequence:** We get *more* software, not *cheaper* software.
*   **Bottleneck:** Shifted from **Creation** $\to$ **Verification**.
*   **Risk:** "Slop" — code that looks correct but introduces tech debt.

> **Case Study:** AI agents generate legacy codebases at lightspeed without guardrails.

---

## The Harness vs. The Model

Better models yield diminishing returns without a robust runtime.

*   **The Model:** The CPU (Raw Intelligence).
*   **The Harness:** The OS (Safety, Context, Tool Loops).
*   **Example:** Code agents as "Computer Use Agents for Terminal," not just models.

> **Key Insight:** Open-source the harness (SDK) so devs don't rebuild runtime for every model update.

---

<!-- _class: invert -->

# Part 2: Cognitive Architectures
## Managing Context & Complexity

---

## The "Dumb Zone" & Context Compaction

Transformer attention degrades when context is filled with irrelevant tokens.

*   **Research Finding:** If $>40\%$ of context is "noise," reasoning collapses.
*   **Solution:** Aggressive **Context Compaction**.
    *   *Research $\to$ Plan $\to$ Implement* loops.
    *   Sub-agents return *only* relevant snippets.

> **Takeaway:** Don't stuff the window. Garbage collect memory dynamically.

---

## From "Agents" to "Skills"

General-purpose agents struggle with domain-specific nuances.

*   **Best Practice:** Stop building "God Agents." Build **Skills**.
*   **Skill:** Scripts/prompts loaded into context *only* when needed.
*   **Analogy:**
    *   **Model:** Raw IQ / Processing Power.
    *   **Skill:** Procedural Muscle Memory.

> **Result:** Modular, composable knowledge rather than one giant system prompt.

---

<!-- _class: invert -->

# Part 3: Training Methodology
## Beyond Prompt Engineering

---

## Agent Reinforcement Fine-Tuning (RFT)

Moving from Prompt Engineering to **Weight Engineering**.

*   **Innovation:** Models interact with tools (grep, ls, curl) *during* training.
*   **Methodology:**
    1.  Model explores tool usage paths.
    2.  Receive reward signal (Did tests pass?).
    3.  Backpropagate on successful trajectory.
*   **Results:** Can reduce tool loops by 50%.

> **Key Insight:** ~100-150 trajectories can yield significant gains.

---

## System Prompt Optimization

Is Reinforcement Learning overkill for your task?

*   **Approach:** Treat System Prompt as a hyperparameter to optimize.
*   **Workflow:**
    1.  Run small model on coding tasks.
    2.  **LLM-as-a-Judge** critiques failures.
    3.  Algorithm rewrites System Prompt based on critiques.

> **Result:** 15% boost on SWE-bench with **zero weight updates**.

---

## Efficient RL Systems

How do we train on millions of trajectories without burning cash?

*   **Bottleneck:** Synchronous RL is inefficient. GPUs idle waiting for slowest sample.
*   **Solution:** **Asynchronous Pipeline RL**.
    *   Decouple generation (actors) from training (learners).
    *   *Trade-off:* **Staleness** — gradients from older policy version.

> **The Math:** Managing **Importance Ratio** variance to prevent divergence.

---

## The "Environment" as Research Primitive

Moving away from "Model Checkpoints" as the unit of research.

*   **Concept:** Environment = Harness + Task + Reward Function.
*   **Standardized RL Labs:**
    *   Standardizing Agent ↔ Interpreter interface.
    *   Code interpreter as a **gym environment** (observation, action, reward).

> **Shift:** From "New Architecture" to "Better Reward Modeling" and "Harder Environments."

---

## Interleaved Reasoning

How do small models (10B) outperform large models (70B+)?

*   **Pattern:** **Interleaved Thinking**.
    *   Standard: `Prompt` $\to$ `Code`.
    *   Interleaved: `Think` $\to$ `Tool` $\to$ `Observe` $\to$ `Think` $\to$ `Code`.
*   **Mechanism:** Model pauses, executes command (ls, grep), reads output, continues.

> **Result:** "System 2" thinking from "System 1" models via tool loops.

---

<!-- _class: invert -->

# Part 4: Next-Gen Modeling
## What are we actually modeling?

---

## Code World Models

Current LLMs predict syntax tokens. Recent research argues this is insufficient.

*   **Concept:** Predict **Execution Traces**, not just Syntax.
*   **Mechanism:** Predict runtime state (memory, variables) at step $t+1$.
*   **Implication:** "Parrot" (repeats syntax) vs. "Programmer" (simulates state machine).

> **Application:** Solving the Halting Problem via approximation.

---

<!-- _class: invert -->

# Part 5: Evaluation & Rigor
## The Data Science Reality Check

---

## The Benchmark Paradox

*   **Observation:** Benchmarks claim AI is superhuman.
*   **Reality:** Expert devs were **19% slower** using AI on complex tasks.
*   **Hypothesis:**
    *   Benchmarks measure *Peak Capability*.
    *   Economics measure *Reliability & Flow*.

> For senior devs, prompting breaks flow — verification time exceeds doing it manually.

---

## Dynamic Evals & Reward Hacking

Static benchmarks suffer from **Data Contamination** (memorized test set).

*   **LiveCodeBench approach:** Evals must update monthly with *new* problems.
*   **Reward Hacking:** Models are lazy.
    *   *Example:* Model hard-coded timestamps in logs to fool the grader.

> **Lesson:** Graders must be adversarial.

---

<!-- _class: invert -->

# Part 6: The Future Interface
## The Death of the IDE

---

## Vertical Integration of the Editor

Traditional IDEs are designed for human typing (ms), not AI token streams (μs).

*   **Agent Manager Pattern:**
    *   Dedicated surface for async, long-horizon work *above* the editor.
    *   Built for agent-first workflows.
*   **Deep Integration:** Control terminal, windowing, render loop — agents "see" system state directly.

> The IDE of the future is built around agent workflows, not human keystrokes.

---

<!-- _class: invert -->

# Class Exercise: Prompt Optimization
## Using Gemini 3 Pro to Optimize Prompts for 2.5 Flash-Lite

---

## The Setup: Judge vs. Student

**Goal:** Use expensive model to optimize prompts for cheap model.

| Role | Model | Cost | Strength |
|------|-------|------|----------|
| **Judge** | Gemini 3 Pro | $2/$12 per 1M | Best reasoning |
| **Student** | Gemini 2.5 Flash-Lite | $0.10/$0.40 per 1M | Fastest, cheapest |

> **Economics:** Pay once to optimize $\to$ Save on every inference forever.

---

## The Workflow

```
1. Define task + evaluation criteria
2. Write initial system prompt for Flash-Lite
3. Run Flash-Lite on test cases
4. Gemini 3 Pro analyzes failures
5. Gemini 3 Pro rewrites the prompt
6. Repeat until convergence
```

> **Key Insight:** Flash-Lite is 30-120x cheaper. 10% quality boost = massive ROI at scale.

---

## Example Tasks for Optimization

Pick one for today's exercise:

**1. Structured Data Extraction**
Extract product info from HTML into JSON. Eval: schema compliance + accuracy.

**2. Code Review Classification**
Classify PRs: `bug_fix`, `feature`, `refactor`, `docs`, `test`. Eval: F1 score.

**3. Croatian-English Translation**
Translate technical docs. Eval: BLEU + terminology ("funkcija" $\to$ "function").

---

<!-- _class: lead -->

# Q&A
### Questions & Discussion
